{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37464bitcodepipenv736823a784cc48d48af916fcf65f3d15",
   "display_name": "Python 3.7.4 64-bit ('code': pipenv)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from collections import Counter\n",
    "from gensim.models import word2vec\n",
    "from janome.tokenizer import Tokenizer\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    \"\"\" 前処理を行う関数\n",
    "\n",
    "    Arg:\n",
    "        text(str)\n",
    "    Return:\n",
    "        text(str)\n",
    "    \"\"\"\n",
    "    text = re.split('\\-{5,}',text)[2]\n",
    "    text = re.split('底本：',text)[0]\n",
    "    text = text.replace('|', '')\n",
    "    text = re.sub('《.+?》', '', text)\n",
    "    text = re.sub('［＃.+?］', '',text)\n",
    "    text = re.sub('\\n\\n', '\\n', text) \n",
    "    text = re.sub('\\r', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# 芥川龍之介「羅生門」\n",
    "url = 'https://www.aozora.gr.jp/cards/000879/files/127_ruby_150.zip'\n",
    "zip = '127_ruby_150.zip'\n",
    "\n",
    "urllib.request.urlretrieve(url, zip)\n",
    "\n",
    "with zipfile.ZipFile(zip, 'r') as myzip:\n",
    "    myzip.extractall()\n",
    "    for myfile in myzip.infolist():\n",
    "        with open(myfile.filename, encoding='sjis') as file:\n",
    "            text = file.read()\n",
    "\n",
    "text = preprocessing(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_words(text):\n",
    "    \"\"\"janomeから名詞・動詞・形容詞を抽出する関数\n",
    "\n",
    "    Args:\n",
    "        text(str)\n",
    "    \"\"\"\n",
    "    tokens = t.tokenize(text)\n",
    "    return [token.base_form for token in tokens \n",
    "        if token.part_of_speech.split(',')[0] in ['名詞', '動詞']]\n",
    "\n",
    "\n",
    "t = Tokenizer()\n",
    "sentences = text.split('。')\n",
    "word_list = [extract_words(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[-0.12435774 -0.0928296  -0.21227041  0.01168441 -0.10422797  0.01936843\n  0.16892774  0.14822248  0.04647612  0.03463526  0.01368704  0.18922411\n  0.11375152  0.44850913 -0.12615559  0.0942183   0.02484121 -0.03254252\n -0.15731055  0.17091514 -0.09072034  0.2361008   0.09366103 -0.21231578\n  0.23379286  0.00428978  0.12654658 -0.12266115 -0.10998385  0.2095599\n -0.34598356  0.19161855  0.12258151 -0.09592339 -0.18811984  0.13793857\n  0.19312674  0.21203391  0.049816   -0.19755103  0.12400132  0.09887454\n -0.07942662  0.12045787 -0.12915029 -0.04250821  0.20540248 -0.3226749\n  0.03510875 -0.158841   -0.11870439 -0.02194624  0.11943144 -0.07560064\n -0.29610786  0.05276647 -0.2710228  -0.07886239 -0.03009902  0.01744395\n  0.00602108  0.09953728  0.2343249   0.03319759 -0.08138849 -0.18943223\n -0.01175497 -0.10421355  0.10447129 -0.15304336  0.15124056 -0.26566738\n  0.07428207  0.16894765 -0.3247511  -0.11495972 -0.16316305 -0.2573537\n  0.01653839  0.0443515  -0.0187621   0.05045    -0.10622831 -0.1724598\n -0.06958862  0.20703292  0.13369636  0.19654334  0.10842206  0.06228387\n -0.1661809   0.2308986  -0.18370523 -0.07289337 -0.14454801  0.06394008\n  0.07986897  0.02737451  0.2716167  -0.00523588]\n"
    }
   ],
   "source": [
    "# 学習\n",
    "model = word2vec.Word2Vec(word_list, size=100, min_count=5, window=5, iter=100)\n",
    "\n",
    "# ベクトル確認\n",
    "print(model.__dict__['wv']['羅生門'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "C:\\Users\\hunkn\\.virtualenvs\\code-WBaFQdnG\\lib\\site-packages\\ipykernel_launcher.py:32: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
    },
    {
     "ename": "KeyError",
     "evalue": "\"word 'r' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-d33b49741543>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[0mpca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.virtualenvs\\code-WBaFQdnG\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36mnew_func1\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1445\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1446\u001b[0m                 )\n\u001b[1;32m-> 1447\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1448\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1449\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mnew_func1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.virtualenvs\\code-WBaFQdnG\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, words)\u001b[0m\n\u001b[0;32m   1119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1120\u001b[0m         \"\"\"\n\u001b[1;32m-> 1121\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Method will be removed in 4.0.0, use self.wv.__contains__() instead\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.virtualenvs\\code-WBaFQdnG\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, entities)\u001b[0m\n\u001b[0;32m    351\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m             \u001b[1;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.virtualenvs\\code-WBaFQdnG\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 471\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    472\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.virtualenvs\\code-WBaFQdnG\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[1;34m(self, word, use_norm)\u001b[0m\n\u001b[0;32m    466\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"word 'r' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "def append_words(view_words):\n",
    "    \"\"\"可視化する単語を追加する関数\n",
    "    \n",
    "    Args:\n",
    "        words(list)\n",
    "    Return:\n",
    "        words(list)\n",
    "    \"\"\"\n",
    "    wrods = []\n",
    "    for i in range(len(view_words)):\n",
    "        words.append([view_words[i],\"r\"])\n",
    "\n",
    "    return words\n",
    "\n",
    "\n",
    "font_path = 'data/TakaoPGothic.ttf'  # 日本語フォント\n",
    "font_prop = FontProperties(fname=font_path)\n",
    "\n",
    "# 手動で描画したい単語をリストに入れること\n",
    "view_words = ['羅生門', '下人', '男', '鴉', '雨', '死骸', '老婆', '太刀', '髪の毛', '饑死', '作者', '石段', '京都']\n",
    "\n",
    "# TODO:単語をランキングにして自動化\n",
    "# word_list_onevec = list(itertools.chain.from_iterable(word_list))\n",
    "# view_words = Counter(word_list_onevec)\n",
    "\n",
    "words = []\n",
    "words = append_words(view_words)\n",
    "\n",
    "data = []\n",
    "for i in range(len(words)):\n",
    "    for j in range(len(words[i][0])):\n",
    "        data.append(model[words[i][j]])\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(data)\n",
    "data_pca = pca.transform(data)\n",
    "\n",
    "# plot\n",
    "for i in range(len(data_pca)):\n",
    "    plt.plot(data_pca[i][0], data_pca[i][1], ms=5.0, zorder=2, marker=\"x\", color=words[i][1])  # points\n",
    "    plt.annotate(words[i][0], (data_pca[i][0], data_pca[i][1]), size=7, fontproperties=font_prop)  # texts\n",
    "\n",
    "plt.title('Word2Vec Plot')\n",
    "plt.savefig('data/word2vec_plot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}